{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0590fcce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a325da97-6c47-46c9-96a0-a1fccad92aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.8.3)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,923 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [32.9 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [36.0 kB]\n",
            "Fetched 2,424 kB in 1s (1,621 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 libudev1 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 7 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.16 [76.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.16 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 2s (17.7 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.16) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126484 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.16_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.16) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.16) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126713 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.16) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt-get install chromium-browser -y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current Page Only\n"
      ],
      "metadata": {
        "id": "U6OJpBfT3qr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium -q\n",
        "!apt-get update -q\n",
        "!apt-get install chromium-browser -y\n",
        "\n",
        "# Enhanced Offline Webpage Archiver for Google Colab\n",
        "# Handles lazy loading, loading screens, and dynamic content\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import zipfile\n",
        "import requests\n",
        "from urllib.parse import urljoin, urlparse, quote\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import base64\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Install required packages\n",
        "!pip install selenium beautifulsoup4 requests pillow -q\n",
        "\n",
        "# Setup Chrome WebDriver for Colab\n",
        "!apt-get update >/dev/null 2>&1\n",
        "!apt-get install -y chromium-browser >/dev/null 2>&1\n",
        "\n",
        "class EnhancedWebpageArchiver:\n",
        "    def __init__(self, base_url, wait_time=10, scroll_pause=2):\n",
        "        self.base_url = base_url\n",
        "        self.domain = urlparse(base_url).netloc\n",
        "        self.archive_dir = f\"archive_{self.domain}_{int(time.time())}\"\n",
        "        self.assets_dir = os.path.join(self.archive_dir, \"assets\")\n",
        "        self.downloaded_urls = set()\n",
        "        self.url_mapping = {}\n",
        "        self.wait_time = wait_time\n",
        "        self.scroll_pause = scroll_pause\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.archive_dir, exist_ok=True)\n",
        "        os.makedirs(self.assets_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"css\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"js\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"images\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"fonts\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"other\"), exist_ok=True)\n",
        "\n",
        "        # Setup Selenium WebDriver\n",
        "        self.setup_webdriver()\n",
        "\n",
        "    def setup_webdriver(self):\n",
        "        \"\"\"Setup Chrome WebDriver with enhanced options for dynamic content\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        chrome_options.add_argument(\"--disable-gpu\")\n",
        "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "        chrome_options.add_argument(\"--disable-web-security\")\n",
        "        chrome_options.add_argument(\"--allow-running-insecure-content\")\n",
        "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        # Disable images initially to load faster, we'll enable them later\n",
        "        prefs = {\n",
        "            \"profile.managed_default_content_settings.images\": 1,\n",
        "            \"profile.default_content_setting_values.notifications\": 2\n",
        "        }\n",
        "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        self.driver.set_page_load_timeout(60)  # Increased timeout\n",
        "\n",
        "    def wait_for_page_load(self):\n",
        "        \"\"\"Enhanced waiting mechanism for page load and dynamic content\"\"\"\n",
        "        print(\"⏳ Waiting for initial page load...\")\n",
        "\n",
        "        try:\n",
        "            # Wait for basic DOM to be ready\n",
        "            WebDriverWait(self.driver, self.wait_time).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "\n",
        "            # Wait for document ready state\n",
        "            WebDriverWait(self.driver, self.wait_time).until(\n",
        "                lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\"\n",
        "            )\n",
        "\n",
        "            print(\"✅ Basic page load complete\")\n",
        "\n",
        "            # Check for common loading indicators and wait for them to disappear\n",
        "            loading_selectors = [\n",
        "                '[class*=\"loading\"]',\n",
        "                '[class*=\"loader\"]',\n",
        "                '[class*=\"spinner\"]',\n",
        "                '[id*=\"loading\"]',\n",
        "                '[id*=\"loader\"]',\n",
        "                '.loading-screen',\n",
        "                '.loader-container',\n",
        "                '.spinner-container',\n",
        "                '[class*=\"preloader\"]',\n",
        "                '[class*=\"load-mask\"]',\n",
        "                '.loading-overlay'\n",
        "            ]\n",
        "\n",
        "            print(\"🔍 Checking for loading screens...\")\n",
        "            for selector in loading_selectors:\n",
        "                try:\n",
        "                    loading_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    if loading_elements:\n",
        "                        print(f\"Found loading element: {selector}, waiting for it to disappear...\")\n",
        "                        WebDriverWait(self.driver, 15).until(\n",
        "                            EC.invisibility_of_element_located((By.CSS_SELECTOR, selector))\n",
        "                        )\n",
        "                        print(f\"✅ Loading element {selector} disappeared\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Additional wait for JavaScript to finish\n",
        "            print(\"⏳ Waiting for JavaScript to finish...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "            # Wait for any pending network requests (check for active XHR)\n",
        "            self.wait_for_network_idle()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Page load wait completed with some issues: {str(e)}\")\n",
        "            print(\"Continuing with current page state...\")\n",
        "\n",
        "    def wait_for_network_idle(self):\n",
        "        \"\"\"Wait for network to be idle (no pending requests)\"\"\"\n",
        "        try:\n",
        "            # Check for jQuery if it exists\n",
        "            jquery_active = self.driver.execute_script(\"\"\"\n",
        "                if (typeof jQuery !== 'undefined') {\n",
        "                    return jQuery.active;\n",
        "                }\n",
        "                return 0;\n",
        "            \"\"\")\n",
        "\n",
        "            if jquery_active > 0:\n",
        "                print(f\"⏳ Waiting for {jquery_active} jQuery requests to complete...\")\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    lambda driver: driver.execute_script(\"return typeof jQuery !== 'undefined' ? jQuery.active : 0\") == 0\n",
        "                )\n",
        "\n",
        "            # Wait a bit more for any remaining async operations\n",
        "            time.sleep(2)\n",
        "            print(\"✅ Network appears idle\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Network idle check completed: {str(e)}\")\n",
        "\n",
        "    def handle_lazy_loading(self):\n",
        "        \"\"\"Trigger lazy loading by scrolling through the page\"\"\"\n",
        "        print(\"🔄 Handling lazy loading by scrolling...\")\n",
        "\n",
        "        try:\n",
        "            # Get initial page height\n",
        "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            scroll_position = 0\n",
        "\n",
        "            while True:\n",
        "                # Scroll down in increments\n",
        "                scroll_increment = 500\n",
        "                scroll_position += scroll_increment\n",
        "\n",
        "                self.driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
        "                time.sleep(self.scroll_pause)\n",
        "\n",
        "                # Check if we've reached the bottom\n",
        "                current_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "                window_height = self.driver.execute_script(\"return window.innerHeight\")\n",
        "\n",
        "                if scroll_position >= current_height - window_height:\n",
        "                    # We've reached the bottom, check if content has grown\n",
        "                    time.sleep(2)  # Wait for any lazy content to load\n",
        "                    new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "                    if new_height == current_height:\n",
        "                        # No more content loaded, we're done\n",
        "                        break\n",
        "                    else:\n",
        "                        # More content loaded, continue\n",
        "                        last_height = new_height\n",
        "                        print(f\"📈 Page height increased to {new_height}px, continuing...\")\n",
        "\n",
        "            # Scroll back to top\n",
        "            self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Try to trigger any remaining lazy-loaded images\n",
        "            self.trigger_lazy_images()\n",
        "\n",
        "            print(\"✅ Lazy loading handling complete\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Lazy loading handling completed with issues: {str(e)}\")\n",
        "\n",
        "    def trigger_lazy_images(self):\n",
        "        \"\"\"Specifically trigger lazy-loaded images\"\"\"\n",
        "        try:\n",
        "            # Common lazy loading attributes\n",
        "            lazy_selectors = [\n",
        "                'img[data-src]',\n",
        "                'img[data-lazy]',\n",
        "                'img[data-original]',\n",
        "                'img[loading=\"lazy\"]',\n",
        "                'img[data-srcset]',\n",
        "                '[data-bg]',\n",
        "                '[data-background]'\n",
        "            ]\n",
        "\n",
        "            for selector in lazy_selectors:\n",
        "                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                for element in elements:\n",
        "                    try:\n",
        "                        # Scroll to element to trigger loading\n",
        "                        self.driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
        "                        time.sleep(0.5)\n",
        "\n",
        "                        # Try to trigger loading by hovering\n",
        "                        ActionChains(self.driver).move_to_element(element).perform()\n",
        "                        time.sleep(0.5)\n",
        "\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Final wait for images to load\n",
        "            time.sleep(3)\n",
        "            print(\"✅ Lazy image loading triggered\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Lazy image triggering completed: {str(e)}\")\n",
        "\n",
        "    def wait_for_specific_content(self, content_selectors=None):\n",
        "        \"\"\"Wait for specific content elements to appear\"\"\"\n",
        "        if not content_selectors:\n",
        "            # Default selectors for common content\n",
        "            content_selectors = [\n",
        "                'main',\n",
        "                'article',\n",
        "                '.content',\n",
        "                '#content',\n",
        "                '.main-content',\n",
        "                '.post-content',\n",
        "                '.article-body'\n",
        "            ]\n",
        "\n",
        "        print(\"🎯 Waiting for main content to appear...\")\n",
        "\n",
        "        for selector in content_selectors:\n",
        "            try:\n",
        "                element = WebDriverWait(self.driver, 5).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
        "                )\n",
        "                if element and element.text.strip():\n",
        "                    print(f\"✅ Found content with selector: {selector}\")\n",
        "                    return True\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(\"⚠️ Specific content selectors not found, proceeding anyway...\")\n",
        "        return False\n",
        "\n",
        "    def sanitize_filename(self, filename):\n",
        "        \"\"\"Sanitize filename for file system\"\"\"\n",
        "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
        "        filename = filename[:200]\n",
        "        return filename or \"unnamed_file\"\n",
        "\n",
        "    def get_file_extension(self, url, content_type=None):\n",
        "        \"\"\"Determine file extension from URL or content type\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        if path.endswith(('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg',\n",
        "                         '.woff', '.woff2', '.ttf', '.eot', '.ico', '.pdf', '.mp4', '.mp3')):\n",
        "            return os.path.splitext(path)[1]\n",
        "\n",
        "        if content_type:\n",
        "            content_type = content_type.lower()\n",
        "            if 'css' in content_type:\n",
        "                return '.css'\n",
        "            elif 'javascript' in content_type:\n",
        "                return '.js'\n",
        "            elif 'image/png' in content_type:\n",
        "                return '.png'\n",
        "            elif 'image/jpeg' in content_type:\n",
        "                return '.jpg'\n",
        "            elif 'image/gif' in content_type:\n",
        "                return '.gif'\n",
        "            elif 'image/svg' in content_type:\n",
        "                return '.svg'\n",
        "            elif 'font/woff' in content_type:\n",
        "                return '.woff'\n",
        "            elif 'font/woff2' in content_type:\n",
        "                return '.woff2'\n",
        "\n",
        "        return '.html'\n",
        "\n",
        "    def download_asset(self, url, referer_url=None):\n",
        "        \"\"\"Download an asset and return the local path\"\"\"\n",
        "        if url in self.downloaded_urls:\n",
        "            return self.url_mapping.get(url)\n",
        "\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            if referer_url:\n",
        "                headers['Referer'] = referer_url\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Determine file type and directory\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            extension = self.get_file_extension(url, content_type)\n",
        "\n",
        "            if extension in ['.css']:\n",
        "                subdir = \"css\"\n",
        "            elif extension in ['.js']:\n",
        "                subdir = \"js\"\n",
        "            elif extension in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg', '.ico']:\n",
        "                subdir = \"images\"\n",
        "            elif extension in ['.woff', '.woff2', '.ttf', '.eot']:\n",
        "                subdir = \"fonts\"\n",
        "            else:\n",
        "                subdir = \"other\"\n",
        "\n",
        "            # Create filename\n",
        "            parsed_url = urlparse(url)\n",
        "            filename = os.path.basename(parsed_url.path) or \"index\"\n",
        "            filename = self.sanitize_filename(filename)\n",
        "\n",
        "            if not filename.endswith(extension):\n",
        "                filename += extension\n",
        "\n",
        "            # Ensure unique filename\n",
        "            counter = 1\n",
        "            original_filename = filename\n",
        "            while os.path.exists(os.path.join(self.assets_dir, subdir, filename)):\n",
        "                name, ext = os.path.splitext(original_filename)\n",
        "                filename = f\"{name}_{counter}{ext}\"\n",
        "                counter += 1\n",
        "\n",
        "            local_path = os.path.join(self.assets_dir, subdir, filename)\n",
        "\n",
        "            # Download file\n",
        "            with open(local_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "            # Store mapping\n",
        "            relative_path = f\"assets/{subdir}/{filename}\"\n",
        "            self.downloaded_urls.add(url)\n",
        "            self.url_mapping[url] = relative_path\n",
        "\n",
        "            print(f\"Downloaded: {url} -> {relative_path}\")\n",
        "\n",
        "            # If it's CSS, process it for additional assets\n",
        "            if extension == '.css':\n",
        "                self.process_css_file(local_path, url)\n",
        "\n",
        "            return relative_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_css_file(self, css_path, css_url):\n",
        "        \"\"\"Process CSS file to download referenced assets\"\"\"\n",
        "        try:\n",
        "            with open(css_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                css_content = f.read()\n",
        "\n",
        "            # Find URLs in CSS\n",
        "            url_pattern = r'url\\([\"\\']?(.*?)[\"\\']?\\)'\n",
        "            urls = re.findall(url_pattern, css_content, re.IGNORECASE)\n",
        "\n",
        "            modified_css = css_content\n",
        "\n",
        "            for asset_url in urls:\n",
        "                # Skip data URLs\n",
        "                if asset_url.startswith('data:'):\n",
        "                    continue\n",
        "\n",
        "                # Convert to absolute URL\n",
        "                absolute_url = urljoin(css_url, asset_url)\n",
        "\n",
        "                # Download asset\n",
        "                local_path = self.download_asset(absolute_url, css_url)\n",
        "                if local_path:\n",
        "                    # Update CSS content\n",
        "                    modified_css = modified_css.replace(f'url({asset_url})', f'url(../{local_path})')\n",
        "                    modified_css = modified_css.replace(f'url(\"{asset_url}\")', f'url(\"../{local_path}\")')\n",
        "                    modified_css = modified_css.replace(f\"url('{asset_url}')\", f\"url('../{local_path}')\")\n",
        "\n",
        "            # Write modified CSS\n",
        "            with open(css_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(modified_css)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing CSS file {css_path}: {str(e)}\")\n",
        "\n",
        "    def process_html_content(self, html_content, base_url):\n",
        "        \"\"\"Process HTML content and download all assets\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Process different types of assets\n",
        "        asset_tags = [\n",
        "            ('link', 'href', ['stylesheet', 'icon', 'shortcut icon']),\n",
        "            ('script', 'src', None),\n",
        "            ('img', 'src', None),\n",
        "            ('img', 'data-src', None),  # Lazy loaded images\n",
        "            ('img', 'data-original', None),  # Another lazy loading pattern\n",
        "            ('source', 'src', None),\n",
        "            ('source', 'srcset', None),\n",
        "            ('video', 'src', None),\n",
        "            ('audio', 'src', None),\n",
        "            ('embed', 'src', None),\n",
        "            ('object', 'data', None),\n",
        "            ('iframe', 'src', None),\n",
        "        ]\n",
        "\n",
        "        for tag_name, attr_name, rel_types in asset_tags:\n",
        "            tags = soup.find_all(tag_name)\n",
        "\n",
        "            for tag in tags:\n",
        "                # Check rel attribute if specified\n",
        "                if rel_types and tag_name == 'link':\n",
        "                    rel = tag.get('rel', [])\n",
        "                    if isinstance(rel, str):\n",
        "                        rel = [rel]\n",
        "                    if not any(r in rel_types for r in rel):\n",
        "                        continue\n",
        "\n",
        "                asset_url = tag.get(attr_name)\n",
        "                if not asset_url or asset_url.startswith('data:') or asset_url.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                # Handle srcset attribute (multiple URLs)\n",
        "                if attr_name == 'srcset':\n",
        "                    srcset_urls = []\n",
        "                    for src_desc in asset_url.split(','):\n",
        "                        src_url = src_desc.strip().split()[0]\n",
        "                        absolute_url = urljoin(base_url, src_url)\n",
        "                        local_path = self.download_asset(absolute_url, base_url)\n",
        "                        if local_path:\n",
        "                            srcset_urls.append(src_desc.replace(src_url, local_path))\n",
        "                    if srcset_urls:\n",
        "                        tag[attr_name] = ', '.join(srcset_urls)\n",
        "                else:\n",
        "                    # Convert to absolute URL\n",
        "                    absolute_url = urljoin(base_url, asset_url)\n",
        "\n",
        "                    # Download asset\n",
        "                    local_path = self.download_asset(absolute_url, base_url)\n",
        "                    if local_path:\n",
        "                        tag[attr_name] = local_path\n",
        "\n",
        "                # For lazy-loaded images, also set the src attribute\n",
        "                if tag_name == 'img' and attr_name in ['data-src', 'data-original'] and local_path:\n",
        "                    tag['src'] = local_path\n",
        "\n",
        "        # Process inline styles\n",
        "        for tag in soup.find_all(style=True):\n",
        "            style_content = tag['style']\n",
        "            # Find URLs in inline styles\n",
        "            url_pattern = r'url\\([\"\\']?(.*?)[\"\\']?\\)'\n",
        "            urls = re.findall(url_pattern, style_content, re.IGNORECASE)\n",
        "\n",
        "            for asset_url in urls:\n",
        "                if asset_url.startswith('data:'):\n",
        "                    continue\n",
        "\n",
        "                absolute_url = urljoin(base_url, asset_url)\n",
        "                local_path = self.download_asset(absolute_url, base_url)\n",
        "                if local_path:\n",
        "                    style_content = style_content.replace(asset_url, local_path)\n",
        "\n",
        "            tag['style'] = style_content\n",
        "\n",
        "        # Process style tags\n",
        "        for style_tag in soup.find_all('style'):\n",
        "            if style_tag.string:\n",
        "                css_content = style_tag.string\n",
        "                url_pattern = r'url\\([\"\\']?(.*?)[\"\\']?\\)'\n",
        "                urls = re.findall(url_pattern, css_content, re.IGNORECASE)\n",
        "\n",
        "                for asset_url in urls:\n",
        "                    if asset_url.startswith('data:'):\n",
        "                        continue\n",
        "\n",
        "                    absolute_url = urljoin(base_url, asset_url)\n",
        "                    local_path = self.download_asset(absolute_url, base_url)\n",
        "                    if local_path:\n",
        "                        css_content = css_content.replace(asset_url, local_path)\n",
        "\n",
        "                style_tag.string = css_content\n",
        "\n",
        "        return str(soup)\n",
        "\n",
        "    def take_screenshot(self):\n",
        "        \"\"\"Take a screenshot of the webpage after content is loaded\"\"\"\n",
        "        try:\n",
        "            print(\"📸 Taking screenshots...\")\n",
        "\n",
        "            # Take screenshot of current viewport\n",
        "            screenshot_path = os.path.join(self.archive_dir, \"screenshot.png\")\n",
        "            self.driver.save_screenshot(screenshot_path)\n",
        "\n",
        "            # Get page dimensions and take full page screenshot\n",
        "            total_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            total_width = self.driver.execute_script(\"return document.body.scrollWidth\")\n",
        "\n",
        "            # Set window size to capture full page\n",
        "            self.driver.set_window_size(max(1920, total_width), total_height)\n",
        "            time.sleep(2)\n",
        "\n",
        "            full_screenshot_path = os.path.join(self.archive_dir, \"screenshot_full.png\")\n",
        "            self.driver.save_screenshot(full_screenshot_path)\n",
        "\n",
        "            # Reset window size\n",
        "            self.driver.set_window_size(1920, 1080)\n",
        "\n",
        "            print(f\"✅ Screenshots saved: {screenshot_path}, {full_screenshot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Failed to take screenshot: {str(e)}\")\n",
        "\n",
        "    def create_archive(self, content_selectors=None):\n",
        "        \"\"\"Create the complete archive with enhanced loading handling\"\"\"\n",
        "        print(f\"🚀 Starting enhanced archive of: {self.base_url}\")\n",
        "        print(f\"📁 Archive directory: {self.archive_dir}\")\n",
        "\n",
        "        try:\n",
        "            # Load page with Selenium first for dynamic content\n",
        "            print(\"🌐 Loading page with Selenium...\")\n",
        "            self.driver.get(self.base_url)\n",
        "\n",
        "            # Enhanced waiting mechanism\n",
        "            self.wait_for_page_load()\n",
        "\n",
        "            # Wait for specific content if selectors provided\n",
        "            if content_selectors:\n",
        "                self.wait_for_specific_content(content_selectors)\n",
        "\n",
        "            # Handle lazy loading\n",
        "            self.handle_lazy_loading()\n",
        "\n",
        "            # Get the final HTML after all content is loaded\n",
        "            final_html = self.driver.page_source\n",
        "\n",
        "            # Take screenshot after everything is loaded\n",
        "            self.take_screenshot()\n",
        "\n",
        "            # Process HTML and download assets\n",
        "            print(\"⚙️ Processing HTML and downloading assets...\")\n",
        "            processed_html = self.process_html_content(final_html, self.base_url)\n",
        "\n",
        "            # Save main HTML file\n",
        "            html_filename = os.path.join(self.archive_dir, \"index.html\")\n",
        "            with open(html_filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(processed_html)\n",
        "\n",
        "            # Create metadata file\n",
        "            page_title = \"No Title\"\n",
        "            try:\n",
        "                title_element = self.driver.find_element(By.TAG_NAME, \"title\")\n",
        "                page_title = title_element.get_attribute(\"text\") or title_element.text or \"No Title\"\n",
        "            except:\n",
        "                soup = BeautifulSoup(final_html, 'html.parser')\n",
        "                if soup.title:\n",
        "                    page_title = soup.title.string or \"No Title\"\n",
        "\n",
        "            metadata = {\n",
        "                'url': self.base_url,\n",
        "                'archived_at': datetime.now().isoformat(),\n",
        "                'title': page_title,\n",
        "                'total_assets': len(self.downloaded_urls),\n",
        "                'page_height': self.driver.execute_script(\"return document.body.scrollHeight\"),\n",
        "                'page_width': self.driver.execute_script(\"return document.body.scrollWidth\"),\n",
        "                'wait_time_used': self.wait_time,\n",
        "                'scroll_pause_used': self.scroll_pause,\n",
        "                'assets': list(self.downloaded_urls)\n",
        "            }\n",
        "\n",
        "            metadata_file = os.path.join(self.archive_dir, \"metadata.json\")\n",
        "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            # Create ZIP archive\n",
        "            print(\"📦 Creating ZIP archive...\")\n",
        "            zip_filename = f\"{self.archive_dir}.zip\"\n",
        "            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for root, dirs, files in os.walk(self.archive_dir):\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        arc_path = os.path.relpath(file_path, self.archive_dir)\n",
        "                        zipf.write(file_path, arc_path)\n",
        "\n",
        "            print(f\"\\n🎉 Archive completed successfully!\")\n",
        "            print(f\"📁 Archive folder: {self.archive_dir}\")\n",
        "            print(f\"📦 ZIP file: {zip_filename}\")\n",
        "            print(f\"🖼️  Screenshots: screenshot.png, screenshot_full.png\")\n",
        "            print(f\"📄 Main file: index.html\")\n",
        "            print(f\"📊 Total assets downloaded: {len(self.downloaded_urls)}\")\n",
        "            print(f\"📏 Page dimensions: {metadata['page_width']}x{metadata['page_height']}px\")\n",
        "\n",
        "            return self.archive_dir, zip_filename\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error creating archive: {str(e)}\")\n",
        "            return None, None\n",
        "        finally:\n",
        "            if hasattr(self, 'driver'):\n",
        "                self.driver.quit()\n",
        "\n",
        "# Enhanced input section\n",
        "print(\"🌐 Enhanced Offline Webpage Archiver\")\n",
        "print(\"📋 Features: Lazy loading support, loading screen detection, dynamic content\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "url = input(\"Enter the URL to archive: \").strip()\n",
        "\n",
        "if url:\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'https://' + url\n",
        "\n",
        "    # Get optional parameters\n",
        "    print(\"\\n⚙️ Optional Settings:\")\n",
        "    wait_time_input = input(\"Max wait time for page load (default 10s): \").strip()\n",
        "    wait_time = int(wait_time_input) if wait_time_input.isdigit() else 10\n",
        "\n",
        "    scroll_pause_input = input(\"Pause between scrolls (default 2s): \").strip()\n",
        "    scroll_pause = float(scroll_pause_input) if scroll_pause_input.replace('.', '').isdigit() else 2.0\n",
        "\n",
        "    content_selectors_input = input(\"Specific content selectors to wait for (comma-separated, optional): \").strip()\n",
        "    content_selectors = [s.strip() for s in content_selectors_input.split(',')] if content_selectors_input else None\n",
        "\n",
        "    print(f\"\\n🚀 Starting enhanced archive process for: {url}\")\n",
        "    print(f\"⏰ Wait time: {wait_time}s, Scroll pause: {scroll_pause}s\")\n",
        "\n",
        "    # Create archiver and start process\n",
        "    archiver = EnhancedWebpageArchiver(url, wait_time, scroll_pause)\n",
        "    archive_dir, zip_file = archiver.create_archive(content_selectors)\n",
        "\n",
        "    if archive_dir:\n",
        "        print(f\"\\n📋 Archive Summary:\")\n",
        "        print(f\"   • Open 'index.html' in the archive folder to view offline\")\n",
        "        print(f\"   • All assets (CSS, JS, images, fonts) are included\")\n",
        "        print(f\"   • Lazy-loaded content has been triggered and captured\")\n",
        "        print(f\"   • Loading screens were detected and waited for\")\n",
        "        print(f\"   • Screenshots show the final loaded state\")\n",
        "        print(f\"   • ZIP file created for easy sharing\")\n",
        "\n",
        "        # List contents\n",
        "        print(f\"\\n📂 Archive contents:\")\n",
        "        for root, dirs, files in os.walk(archive_dir):\n",
        "            level = root.replace(archive_dir, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:10]:  # Limit display\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 10:\n",
        "                print(f\"{subindent}... and {len(files) - 10} more files\")\n",
        "    else:\n",
        "        print(\"❌ Archive process failed. Please check the URL and try again.\")\n",
        "else:\n",
        "    print(\"❌ Please provide a valid URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "collapsed": true,
        "id": "yFpCw8GQ0zY2",
        "outputId": "ced052ab-d517-484e-85b4-e9e349473dd0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-browser is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "🌐 Enhanced Offline Webpage Archiver\n",
            "📋 Features: Lazy loading support, loading screen detection, dynamic content\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-989850683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the URL to archive: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Page"
      ],
      "metadata": {
        "id": "YSoLeNhz31e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prKt8mUKuNBA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Webpage Archiver\n",
        "#@markdown Enter a URL to save it as a complete offline archive with all assets, rewritten links, and screenshots.\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from zipfile import ZipFile\n",
        "from IPython.display import HTML, display\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Configuration\n",
        "URL = \"https://iffcophulpur.kvs.ac.in/en/photo-gallery/\"  #@param {type:\"string\"}\n",
        "MAX_DEPTH = 1  #@param {type:\"integer\"}\n",
        "DELAY = 1  #@param {type:\"number\"}  # Delay between requests in seconds\n",
        "WAIT_TIME = 3  #@param {type:\"number\"}  # Time to wait for page to load in seconds\n",
        "INCLUDE_EXTERNAL = False  #@param {type:\"boolean\"}  # Include external domains\n",
        "SCREENSHOT_WIDTH = 1280  #@param {type:\"integer\"}  # Screenshot width in pixels\n",
        "SCREENSHOT_HEIGHT = 900  #@param {type:\"integer\"}  # Screenshot height in pixels\n",
        "\n",
        "# Setup directories\n",
        "!mkdir -p /content/archive\n",
        "!mkdir -p /content/archive/assets\n",
        "!mkdir -p /content/archive/screenshots\n",
        "\n",
        "# Setup Chrome options for Selenium\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument(f'--window-size={SCREENSHOT_WIDTH},{SCREENSHOT_HEIGHT}')\n",
        "\n",
        "# Initialize the browser\n",
        "browser = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Track visited URLs to avoid duplicates\n",
        "visited_urls = set()\n",
        "assets_queue = queue.Queue()\n",
        "pages_to_process = queue.Queue()\n",
        "\n",
        "# Function to sanitize filenames\n",
        "def sanitize_filename(filename):\n",
        "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
        "\n",
        "# Function to get domain from URL\n",
        "def get_domain(url):\n",
        "    parsed_uri = urlparse(url)\n",
        "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
        "    return domain\n",
        "\n",
        "# Function to download assets\n",
        "def download_assets():\n",
        "    while True:\n",
        "        asset_info = assets_queue.get()\n",
        "        if asset_info is None:  # Sentinel value to stop the thread\n",
        "            assets_queue.task_done()\n",
        "            break\n",
        "\n",
        "        asset_url, asset_path, asset_folder = asset_info\n",
        "        try:\n",
        "            response = requests.get(asset_url, stream=True, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                # Create directory if it doesn't exist\n",
        "                os.makedirs(os.path.dirname(asset_path), exist_ok=True)\n",
        "\n",
        "                # Save the asset\n",
        "                with open(asset_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(1024):\n",
        "                        f.write(chunk)\n",
        "                print(f\"Downloaded asset: {asset_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading asset {asset_url}: {str(e)}\")\n",
        "\n",
        "        assets_queue.task_done()\n",
        "\n",
        "# Function to process a webpage\n",
        "def process_page(url, depth=0):\n",
        "    if url in visited_urls or depth > MAX_DEPTH:\n",
        "        return\n",
        "\n",
        "    visited_urls.add(url)\n",
        "    print(f\"Processing: {url} (Depth: {depth})\")\n",
        "\n",
        "    try:\n",
        "        # Load the page with Selenium for screenshot and JavaScript rendering\n",
        "        browser.get(url)\n",
        "        time.sleep(WAIT_TIME)\n",
        "\n",
        "        # Take screenshot\n",
        "        parsed_url = urlparse(url)\n",
        "        screenshot_path = f\"/content/archive/screenshots/{sanitize_filename(parsed_url.netloc + parsed_url.path)}.png\"\n",
        "        browser.save_screenshot(screenshot_path)\n",
        "        print(f\"Saved screenshot: {screenshot_path}\")\n",
        "\n",
        "        # Get page source after JavaScript execution\n",
        "        page_source = browser.page_source\n",
        "\n",
        "        # Parse with BeautifulSoup\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Create a local path for the HTML file\n",
        "        parsed_url = urlparse(url)\n",
        "        path = parsed_url.path\n",
        "        if path == '' or path.endswith('/'):\n",
        "            path += 'index.html'\n",
        "        if not path.endswith('.html'):\n",
        "            path += '.html'\n",
        "\n",
        "        local_path = f\"/content/archive/{sanitize_filename(parsed_url.netloc)}{path}\"\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "        # Process all links and assets\n",
        "        domain = get_domain(url)\n",
        "\n",
        "        # Process stylesheets\n",
        "        for link in soup.find_all('link', rel='stylesheet'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                asset_url = urljoin(url, href)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"style_{len(visited_urls)}.css\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/css/{asset_filename}\"\n",
        "                link['href'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/css/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"css\"))\n",
        "\n",
        "        # Process scripts\n",
        "        for script in soup.find_all('script'):\n",
        "            src = script.get('src')\n",
        "            if src:\n",
        "                asset_url = urljoin(url, src)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"script_{len(visited_urls)}.js\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/js/{asset_filename}\"\n",
        "                script['src'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/js/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"js\"))\n",
        "\n",
        "        # Process images\n",
        "        for img in soup.find_all('img'):\n",
        "            src = img.get('src')\n",
        "            if src:\n",
        "                asset_url = urljoin(url, src)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"image_{len(visited_urls)}.png\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/images/{asset_filename}\"\n",
        "                img['src'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/images/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"images\"))\n",
        "\n",
        "        # Process other assets (videos, audio, etc.)\n",
        "        for tag in soup.find_all(['video', 'audio', 'source']):\n",
        "            src = tag.get('src')\n",
        "            if src:\n",
        "                asset_url = urljoin(url, src)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"media_{len(visited_urls)}\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/media/{asset_filename}\"\n",
        "                tag['src'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/media/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"media\"))\n",
        "\n",
        "        # Process internal links for crawling\n",
        "        if depth < MAX_DEPTH:\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link.get('href')\n",
        "                if href and not href.startswith('#') and not href.startswith('javascript:'):\n",
        "                    new_url = urljoin(url, href)\n",
        "                    new_domain = get_domain(new_url)\n",
        "\n",
        "                    # Only follow links from the same domain unless INCLUDE_EXTERNAL is True\n",
        "                    if new_domain == domain or INCLUDE_EXTERNAL:\n",
        "                        pages_to_process.put((new_url, depth + 1))\n",
        "\n",
        "        # Save the modified HTML\n",
        "        with open(local_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(str(soup))\n",
        "        print(f\"Saved page: {local_path}\")\n",
        "\n",
        "        # Add delay to avoid overwhelming the server\n",
        "        time.sleep(DELAY)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "# Start asset download threads\n",
        "num_download_threads = 5\n",
        "download_threads = []\n",
        "for _ in range(num_download_threads):\n",
        "    thread = threading.Thread(target=download_assets)\n",
        "    thread.start()\n",
        "    download_threads.append(thread)\n",
        "\n",
        "# Add the initial URL to the queue\n",
        "pages_to_process.put((URL, 0))\n",
        "\n",
        "# Process pages\n",
        "try:\n",
        "    while not pages_to_process.empty():\n",
        "        url, depth = pages_to_process.get()\n",
        "        process_page(url, depth)\n",
        "        pages_to_process.task_done()\n",
        "\n",
        "        # Small delay to prevent queue checking from consuming too much CPU\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        # Check if we've reached the maximum number of URLs to prevent infinite loops\n",
        "        if len(visited_urls) > 1000:\n",
        "            print(\"Reached maximum number of URLs (1000). Stopping.\")\n",
        "            break\n",
        "finally:\n",
        "    # Stop all download threads\n",
        "    for _ in range(num_download_threads):\n",
        "        assets_queue.put(None)\n",
        "\n",
        "    # Wait for all download threads to finish\n",
        "    for thread in download_threads:\n",
        "        thread.join()\n",
        "\n",
        "    # Close the browser\n",
        "    browser.quit()\n",
        "\n",
        "# Create a ZIP file of the archive\n",
        "domain_name = sanitize_filename(urlparse(URL).netloc)\n",
        "zip_path = f\"/content/{domain_name}_archive.zip\"\n",
        "with ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(\"/content/archive\"):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, \"/content/archive\")\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"Archive created: {zip_path}\")\n",
        "\n",
        "# Create a download link\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n",
        "\n",
        "# Display summary\n",
        "display(HTML(f\"\"\"\n",
        "<h3>Archive Summary</h3>\n",
        "<p><strong>Original URL:</strong> {URL}</p>\n",
        "<p><strong>Pages Archived:</strong> {len(visited_urls)}</p>\n",
        "<p><strong>Archive File:</strong> {domain_name}_archive.zip</p>\n",
        "<p>The archive includes:</p>\n",
        "<ul>\n",
        "  <li>All HTML pages with rewritten links</li>\n",
        "  <li>All assets (CSS, JavaScript, images, etc.)</li>\n",
        "  <li>Screenshots of each page</li>\n",
        "</ul>\n",
        "\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}