{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0590fcce"
      },
      "outputs": [],
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt-get install chromium-browser -y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current Page Only\n"
      ],
      "metadata": {
        "id": "U6OJpBfT3qr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium -q\n",
        "!apt-get update -q\n",
        "!apt-get install chromium-browser -y\n",
        "\n",
        "# Enhanced Offline Webpage Archiver for Google Colab\n",
        "# Handles lazy loading, loading screens, and dynamic content\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import zipfile\n",
        "import requests\n",
        "from urllib.parse import urljoin, urlparse, quote\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import base64\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Install required packages\n",
        "!pip install selenium beautifulsoup4 requests pillow -q\n",
        "\n",
        "# Setup Chrome WebDriver for Colab\n",
        "!apt-get update >/dev/null 2>&1\n",
        "!apt-get install -y chromium-browser >/dev/null 2>&1\n",
        "\n",
        "class EnhancedWebpageArchiver:\n",
        "    def __init__(self, base_url, wait_time=10, scroll_pause=2):\n",
        "        self.base_url = base_url\n",
        "        self.domain = urlparse(base_url).netloc\n",
        "        self.archive_dir = f\"archive_{self.domain}_{int(time.time())}\"\n",
        "        self.assets_dir = os.path.join(self.archive_dir, \"assets\")\n",
        "        self.downloaded_urls = set()\n",
        "        self.url_mapping = {}\n",
        "        self.wait_time = wait_time\n",
        "        self.scroll_pause = scroll_pause\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.archive_dir, exist_ok=True)\n",
        "        os.makedirs(self.assets_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"css\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"js\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"images\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"fonts\"), exist_ok=True)\n",
        "        os.makedirs(os.path.join(self.assets_dir, \"other\"), exist_ok=True)\n",
        "\n",
        "        # Setup Selenium WebDriver\n",
        "        self.setup_webdriver()\n",
        "\n",
        "    def setup_webdriver(self):\n",
        "        \"\"\"Setup Chrome WebDriver with enhanced options for dynamic content\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        chrome_options.add_argument(\"--disable-gpu\")\n",
        "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "        chrome_options.add_argument(\"--disable-web-security\")\n",
        "        chrome_options.add_argument(\"--allow-running-insecure-content\")\n",
        "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        # Disable images initially to load faster, we'll enable them later\n",
        "        prefs = {\n",
        "            \"profile.managed_default_content_settings.images\": 1,\n",
        "            \"profile.default_content_setting_values.notifications\": 2\n",
        "        }\n",
        "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        self.driver.set_page_load_timeout(60)  # Increased timeout\n",
        "\n",
        "    def wait_for_page_load(self):\n",
        "        \"\"\"Enhanced waiting mechanism for page load and dynamic content\"\"\"\n",
        "        print(\"‚è≥ Waiting for initial page load...\")\n",
        "\n",
        "        try:\n",
        "            # Wait for basic DOM to be ready\n",
        "            WebDriverWait(self.driver, self.wait_time).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "            )\n",
        "\n",
        "            # Wait for document ready state\n",
        "            WebDriverWait(self.driver, self.wait_time).until(\n",
        "                lambda driver: driver.execute_script(\"return document.readyState\") == \"complete\"\n",
        "            )\n",
        "\n",
        "            print(\"‚úÖ Basic page load complete\")\n",
        "\n",
        "            # Check for common loading indicators and wait for them to disappear\n",
        "            loading_selectors = [\n",
        "                '[class*=\"loading\"]',\n",
        "                '[class*=\"loader\"]',\n",
        "                '[class*=\"spinner\"]',\n",
        "                '[id*=\"loading\"]',\n",
        "                '[id*=\"loader\"]',\n",
        "                '.loading-screen',\n",
        "                '.loader-container',\n",
        "                '.spinner-container',\n",
        "                '[class*=\"preloader\"]',\n",
        "                '[class*=\"load-mask\"]',\n",
        "                '.loading-overlay'\n",
        "            ]\n",
        "\n",
        "            print(\"üîç Checking for loading screens...\")\n",
        "            for selector in loading_selectors:\n",
        "                try:\n",
        "                    loading_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                    if loading_elements:\n",
        "                        print(f\"Found loading element: {selector}, waiting for it to disappear...\")\n",
        "                        WebDriverWait(self.driver, 15).until(\n",
        "                            EC.invisibility_of_element_located((By.CSS_SELECTOR, selector))\n",
        "                        )\n",
        "                        print(f\"‚úÖ Loading element {selector} disappeared\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Additional wait for JavaScript to finish\n",
        "            print(\"‚è≥ Waiting for JavaScript to finish...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "            # Wait for any pending network requests (check for active XHR)\n",
        "            self.wait_for_network_idle()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Page load wait completed with some issues: {str(e)}\")\n",
        "            print(\"Continuing with current page state...\")\n",
        "\n",
        "    def wait_for_network_idle(self):\n",
        "        \"\"\"Wait for network to be idle (no pending requests)\"\"\"\n",
        "        try:\n",
        "            # Check for jQuery if it exists\n",
        "            jquery_active = self.driver.execute_script(\"\"\"\n",
        "                if (typeof jQuery !== 'undefined') {\n",
        "                    return jQuery.active;\n",
        "                }\n",
        "                return 0;\n",
        "            \"\"\")\n",
        "\n",
        "            if jquery_active > 0:\n",
        "                print(f\"‚è≥ Waiting for {jquery_active} jQuery requests to complete...\")\n",
        "                WebDriverWait(self.driver, 10).until(\n",
        "                    lambda driver: driver.execute_script(\"return typeof jQuery !== 'undefined' ? jQuery.active : 0\") == 0\n",
        "                )\n",
        "\n",
        "            # Wait a bit more for any remaining async operations\n",
        "            time.sleep(2)\n",
        "            print(\"‚úÖ Network appears idle\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Network idle check completed: {str(e)}\")\n",
        "\n",
        "    def handle_lazy_loading(self):\n",
        "        \"\"\"Trigger lazy loading by scrolling through the page\"\"\"\n",
        "        print(\"üîÑ Handling lazy loading by scrolling...\")\n",
        "\n",
        "        try:\n",
        "            # Get initial page height\n",
        "            last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            scroll_position = 0\n",
        "\n",
        "            while True:\n",
        "                # Scroll down in increments\n",
        "                scroll_increment = 500\n",
        "                scroll_position += scroll_increment\n",
        "\n",
        "                self.driver.execute_script(f\"window.scrollTo(0, {scroll_position});\")\n",
        "                time.sleep(self.scroll_pause)\n",
        "\n",
        "                # Check if we've reached the bottom\n",
        "                current_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "                window_height = self.driver.execute_script(\"return window.innerHeight\")\n",
        "\n",
        "                if scroll_position >= current_height - window_height:\n",
        "                    # We've reached the bottom, check if content has grown\n",
        "                    time.sleep(2)  # Wait for any lazy content to load\n",
        "                    new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "                    if new_height == current_height:\n",
        "                        # No more content loaded, we're done\n",
        "                        break\n",
        "                    else:\n",
        "                        # More content loaded, continue\n",
        "                        last_height = new_height\n",
        "                        print(f\"üìà Page height increased to {new_height}px, continuing...\")\n",
        "\n",
        "            # Scroll back to top\n",
        "            self.driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Try to trigger any remaining lazy-loaded images\n",
        "            self.trigger_lazy_images()\n",
        "\n",
        "            print(\"‚úÖ Lazy loading handling complete\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Lazy loading handling completed with issues: {str(e)}\")\n",
        "\n",
        "    def trigger_lazy_images(self):\n",
        "        \"\"\"Specifically trigger lazy-loaded images\"\"\"\n",
        "        try:\n",
        "            # Common lazy loading attributes\n",
        "            lazy_selectors = [\n",
        "                'img[data-src]',\n",
        "                'img[data-lazy]',\n",
        "                'img[data-original]',\n",
        "                'img[loading=\"lazy\"]',\n",
        "                'img[data-srcset]',\n",
        "                '[data-bg]',\n",
        "                '[data-background]'\n",
        "            ]\n",
        "\n",
        "            for selector in lazy_selectors:\n",
        "                elements = self.driver.find_elements(By.CSS_SELECTOR, selector)\n",
        "                for element in elements:\n",
        "                    try:\n",
        "                        # Scroll to element to trigger loading\n",
        "                        self.driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
        "                        time.sleep(0.5)\n",
        "\n",
        "                        # Try to trigger loading by hovering\n",
        "                        ActionChains(self.driver).move_to_element(element).perform()\n",
        "                        time.sleep(0.5)\n",
        "\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            # Final wait for images to load\n",
        "            time.sleep(3)\n",
        "            print(\"‚úÖ Lazy image loading triggered\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Lazy image triggering completed: {str(e)}\")\n",
        "\n",
        "    def wait_for_specific_content(self, content_selectors=None):\n",
        "        \"\"\"Wait for specific content elements to appear\"\"\"\n",
        "        if not content_selectors:\n",
        "            # Default selectors for common content\n",
        "            content_selectors = [\n",
        "                'main',\n",
        "                'article',\n",
        "                '.content',\n",
        "                '#content',\n",
        "                '.main-content',\n",
        "                '.post-content',\n",
        "                '.article-body'\n",
        "            ]\n",
        "\n",
        "        print(\"üéØ Waiting for main content to appear...\")\n",
        "\n",
        "        for selector in content_selectors:\n",
        "            try:\n",
        "                element = WebDriverWait(self.driver, 5).until(\n",
        "                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
        "                )\n",
        "                if element and element.text.strip():\n",
        "                    print(f\"‚úÖ Found content with selector: {selector}\")\n",
        "                    return True\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(\"‚ö†Ô∏è Specific content selectors not found, proceeding anyway...\")\n",
        "        return False\n",
        "\n",
        "    def sanitize_filename(self, filename):\n",
        "        \"\"\"Sanitize filename for file system\"\"\"\n",
        "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
        "        filename = filename[:200]\n",
        "        return filename or \"unnamed_file\"\n",
        "\n",
        "    def get_file_extension(self, url, content_type=None):\n",
        "        \"\"\"Determine file extension from URL or content type\"\"\"\n",
        "        parsed_url = urlparse(url)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        if path.endswith(('.css', '.js', '.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg',\n",
        "                         '.woff', '.woff2', '.ttf', '.eot', '.ico', '.pdf', '.mp4', '.mp3')):\n",
        "            return os.path.splitext(path)[1]\n",
        "\n",
        "        if content_type:\n",
        "            content_type = content_type.lower()\n",
        "            if 'css' in content_type:\n",
        "                return '.css'\n",
        "            elif 'javascript' in content_type:\n",
        "                return '.js'\n",
        "            elif 'image/png' in content_type:\n",
        "                return '.png'\n",
        "            elif 'image/jpeg' in content_type:\n",
        "                return '.jpg'\n",
        "            elif 'image/gif' in content_type:\n",
        "                return '.gif'\n",
        "            elif 'image/svg' in content_type:\n",
        "                return '.svg'\n",
        "            elif 'font/woff' in content_type:\n",
        "                return '.woff'\n",
        "            elif 'font/woff2' in content_type:\n",
        "                return '.woff2'\n",
        "\n",
        "        return '.html'\n",
        "\n",
        "    def download_asset(self, url, referer_url=None):\n",
        "        \"\"\"Download an asset and return the local path\"\"\"\n",
        "        if url in self.downloaded_urls:\n",
        "            return self.url_mapping.get(url)\n",
        "\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "            if referer_url:\n",
        "                headers['Referer'] = referer_url\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=30, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Determine file type and directory\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            extension = self.get_file_extension(url, content_type)\n",
        "\n",
        "            if extension in ['.css']:\n",
        "                subdir = \"css\"\n",
        "            elif extension in ['.js']:\n",
        "                subdir = \"js\"\n",
        "            elif extension in ['.png', '.jpg', '.jpeg', '.gif', '.webp', '.svg', '.ico']:\n",
        "                subdir = \"images\"\n",
        "            elif extension in ['.woff', '.woff2', '.ttf', '.eot']:\n",
        "                subdir = \"fonts\"\n",
        "            else:\n",
        "                subdir = \"other\"\n",
        "\n",
        "            # Create filename\n",
        "            parsed_url = urlparse(url)\n",
        "            filename = os.path.basename(parsed_url.path) or \"index\"\n",
        "            filename = self.sanitize_filename(filename)\n",
        "\n",
        "            if not filename.endswith(extension):\n",
        "                filename += extension\n",
        "\n",
        "            # Ensure unique filename\n",
        "            counter = 1\n",
        "            original_filename = filename\n",
        "            while os.path.exists(os.path.join(self.assets_dir, subdir, filename)):\n",
        "                name, ext = os.path.splitext(original_filename)\n",
        "                filename = f\"{name}_{counter}{ext}\"\n",
        "                counter += 1\n",
        "\n",
        "            local_path = os.path.join(self.assets_dir, subdir, filename)\n",
        "\n",
        "            # Download file\n",
        "            with open(local_path, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "            # Store mapping\n",
        "            relative_path = f\"assets/{subdir}/{filename}\"\n",
        "            self.downloaded_urls.add(url)\n",
        "            self.url_mapping[url] = relative_path\n",
        "\n",
        "            print(f\"Downloaded: {url} -> {relative_path}\")\n",
        "\n",
        "            # If it's CSS, process it for additional assets\n",
        "            if extension == '.css':\n",
        "                self.process_css_file(local_path, url)\n",
        "\n",
        "            return relative_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_css_file(self, css_path, css_url):\n",
        "        \"\"\"Process CSS file to download referenced assets\"\"\"\n",
        "        try:\n",
        "            with open(css_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                css_content = f.read()\n",
        "\n",
        "            # Find URLs in CSS\n",
        "            url_pattern = r'url\\([\"\\']?(.*?)[\"\\']?\\)'\n",
        "            urls = re.findall(url_pattern, css_content, re.IGNORECASE)\n",
        "\n",
        "            modified_css = css_content\n",
        "\n",
        "            for asset_url in urls:\n",
        "                # Skip data URLs\n",
        "                if asset_url.startswith('data:'):\n",
        "                    continue\n",
        "\n",
        "                # Convert to absolute URL\n",
        "                absolute_url = urljoin(css_url, asset_url)\n",
        "\n",
        "                # Download asset\n",
        "                local_path = self.download_asset(absolute_url, css_url)\n",
        "                if local_path:\n",
        "                    # Update CSS content\n",
        "                    modified_css = modified_css.replace(f'url({asset_url})', f'url(../{local_path})')\n",
        "                    modified_css = modified_css.replace(f'url(\"{asset_url}\")', f'url(\"../{local_path}\")')\n",
        "                    modified_css = modified_css.replace(f\"url('{asset_url}')\", f\"url('../{local_path}')\")\n",
        "\n",
        "            # Write modified CSS\n",
        "            with open(css_path, 'w', encoding='utf-8') as f:\n",
        "                f.write(modified_css)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing CSS file {css_path}: {str(e)}\")\n",
        "\n",
        "    def process_html_content(self, html_content, base_url):\n",
        "        \"\"\"Process HTML content and download all assets\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Process different types of assets\n",
        "        asset_tags = [\n",
        "            ('link', 'href', ['stylesheet', 'icon', 'shortcut icon']),\n",
        "            ('script', 'src', None),\n",
        "            ('img', 'src', None),\n",
        "            ('img', 'data-src', None),  # Lazy loaded images\n",
        "            ('img', 'data-original', None),  # Another lazy loading pattern\n",
        "            ('source', 'src', None),\n",
        "            ('source', 'srcset', None),\n",
        "            ('video', 'src', None),\n",
        "            ('audio', 'src', None),\n",
        "            ('embed', 'src', None),\n",
        "            ('object', 'data', None),\n",
        "            ('iframe', 'src', None),\n",
        "        ]\n",
        "\n",
        "        for tag_name, attr_name, rel_types in asset_tags:\n",
        "            tags = soup.find_all(tag_name)\n",
        "\n",
        "            for tag in tags:\n",
        "                # Check rel attribute if specified\n",
        "                if rel_types and tag_name == 'link':\n",
        "                    rel = tag.get('rel', [])\n",
        "                    if isinstance(rel, str):\n",
        "                        rel = [rel]\n",
        "                    if not any(r in rel_types for r in rel):\n",
        "                        continue\n",
        "\n",
        "                asset_url = tag.get(attr_name)\n",
        "                if not asset_url or asset_url.startswith('data:') or asset_url.startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                # Handle srcset attribute (multiple URLs)\n",
        "                if attr_name == 'srcset':\n",
        "                    srcset_urls = []\n",
        "                    for src_desc in asset_url.split(','):\n",
        "                        src_url = src_desc.strip().split()[0]\n",
        "                        absolute_url = urljoin(base_url, src_url)\n",
        "                        local_path = self.download_asset(absolute_url, base_url)\n",
        "                        if local_path:\n",
        "                            srcset_urls.append(src_desc.replace(src_url, local_path))\n",
        "                    if srcset_urls:\n",
        "                        tag[attr_name] = ', '.join(srcset_urls)\n",
        "                else:\n",
        "                    # Convert to absolute URL\n",
        "                    absolute_url = urljoin(base_url, asset_url)\n",
        "\n",
        "                    # Download asset\n",
        "                    local_path = self.download_asset(absolute_url, base_url)\n",
        "                    if local_path:\n",
        "                        tag[attr_name] = local_path\n",
        "\n",
        "                # For lazy-loaded images, also set the src attribute\n",
        "                if tag_name == 'img' and attr_name in ['data-src', 'data-original'] and local_path:\n",
        "                    tag['src'] = local_path\n",
        "\n",
        "        # Process inline styles\n",
        "        for tag in soup.find_all(style=True):\n",
        "            style_content = tag['style']\n",
        "            # Find URLs in inline styles\n",
        "            url_pattern = r'url\\([\"\\']?(.*?)[\"\\']?\\)'\n",
        "            urls = re.findall(url_pattern, style_content, re.IGNORECASE)\n",
        "\n",
        "            for asset_url in urls:\n",
        "                if asset_url.startswith('data:'):\n",
        "                    continue\n",
        "\n",
        "                absolute_url = urljoin(base_url, asset_url)\n",
        "                local_path = self.download_asset(absolute_url, base_url)\n",
        "                if local_path:\n",
        "                    style_content = style_content.replace(asset_url, local_path)\n",
        "\n",
        "            tag['style'] = style_content\n",
        "\n",
        "        # Process style tags\n",
        "        for style_tag in soup.find_all('style'):\n",
        "            if style_tag.string:\n",
        "                css_content = style_tag.string\n",
        "                url_pattern = r'url\\([\"\\']?(.*?)[\"\\']?\\)'\n",
        "                urls = re.findall(url_pattern, css_content, re.IGNORECASE)\n",
        "\n",
        "                for asset_url in urls:\n",
        "                    if asset_url.startswith('data:'):\n",
        "                        continue\n",
        "\n",
        "                    absolute_url = urljoin(base_url, asset_url)\n",
        "                    local_path = self.download_asset(absolute_url, base_url)\n",
        "                    if local_path:\n",
        "                        css_content = css_content.replace(asset_url, local_path)\n",
        "\n",
        "                style_tag.string = css_content\n",
        "\n",
        "        return str(soup)\n",
        "\n",
        "    def take_screenshot(self):\n",
        "        \"\"\"Take a screenshot of the webpage after content is loaded\"\"\"\n",
        "        try:\n",
        "            print(\"üì∏ Taking screenshots...\")\n",
        "\n",
        "            # Take screenshot of current viewport\n",
        "            screenshot_path = os.path.join(self.archive_dir, \"screenshot.png\")\n",
        "            self.driver.save_screenshot(screenshot_path)\n",
        "\n",
        "            # Get page dimensions and take full page screenshot\n",
        "            total_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
        "            total_width = self.driver.execute_script(\"return document.body.scrollWidth\")\n",
        "\n",
        "            # Set window size to capture full page\n",
        "            self.driver.set_window_size(max(1920, total_width), total_height)\n",
        "            time.sleep(2)\n",
        "\n",
        "            full_screenshot_path = os.path.join(self.archive_dir, \"screenshot_full.png\")\n",
        "            self.driver.save_screenshot(full_screenshot_path)\n",
        "\n",
        "            # Reset window size\n",
        "            self.driver.set_window_size(1920, 1080)\n",
        "\n",
        "            print(f\"‚úÖ Screenshots saved: {screenshot_path}, {full_screenshot_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to take screenshot: {str(e)}\")\n",
        "\n",
        "    def create_archive(self, content_selectors=None):\n",
        "        \"\"\"Create the complete archive with enhanced loading handling\"\"\"\n",
        "        print(f\"üöÄ Starting enhanced archive of: {self.base_url}\")\n",
        "        print(f\"üìÅ Archive directory: {self.archive_dir}\")\n",
        "\n",
        "        try:\n",
        "            # Load page with Selenium first for dynamic content\n",
        "            print(\"üåê Loading page with Selenium...\")\n",
        "            self.driver.get(self.base_url)\n",
        "\n",
        "            # Enhanced waiting mechanism\n",
        "            self.wait_for_page_load()\n",
        "\n",
        "            # Wait for specific content if selectors provided\n",
        "            if content_selectors:\n",
        "                self.wait_for_specific_content(content_selectors)\n",
        "\n",
        "            # Handle lazy loading\n",
        "            self.handle_lazy_loading()\n",
        "\n",
        "            # Get the final HTML after all content is loaded\n",
        "            final_html = self.driver.page_source\n",
        "\n",
        "            # Take screenshot after everything is loaded\n",
        "            self.take_screenshot()\n",
        "\n",
        "            # Process HTML and download assets\n",
        "            print(\"‚öôÔ∏è Processing HTML and downloading assets...\")\n",
        "            processed_html = self.process_html_content(final_html, self.base_url)\n",
        "\n",
        "            # Save main HTML file\n",
        "            html_filename = os.path.join(self.archive_dir, \"index.html\")\n",
        "            with open(html_filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(processed_html)\n",
        "\n",
        "            # Create metadata file\n",
        "            page_title = \"No Title\"\n",
        "            try:\n",
        "                title_element = self.driver.find_element(By.TAG_NAME, \"title\")\n",
        "                page_title = title_element.get_attribute(\"text\") or title_element.text or \"No Title\"\n",
        "            except:\n",
        "                soup = BeautifulSoup(final_html, 'html.parser')\n",
        "                if soup.title:\n",
        "                    page_title = soup.title.string or \"No Title\"\n",
        "\n",
        "            metadata = {\n",
        "                'url': self.base_url,\n",
        "                'archived_at': datetime.now().isoformat(),\n",
        "                'title': page_title,\n",
        "                'total_assets': len(self.downloaded_urls),\n",
        "                'page_height': self.driver.execute_script(\"return document.body.scrollHeight\"),\n",
        "                'page_width': self.driver.execute_script(\"return document.body.scrollWidth\"),\n",
        "                'wait_time_used': self.wait_time,\n",
        "                'scroll_pause_used': self.scroll_pause,\n",
        "                'assets': list(self.downloaded_urls)\n",
        "            }\n",
        "\n",
        "            metadata_file = os.path.join(self.archive_dir, \"metadata.json\")\n",
        "            with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            # Create ZIP archive\n",
        "            print(\"üì¶ Creating ZIP archive...\")\n",
        "            zip_filename = f\"{self.archive_dir}.zip\"\n",
        "            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for root, dirs, files in os.walk(self.archive_dir):\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        arc_path = os.path.relpath(file_path, self.archive_dir)\n",
        "                        zipf.write(file_path, arc_path)\n",
        "\n",
        "            print(f\"\\nüéâ Archive completed successfully!\")\n",
        "            print(f\"üìÅ Archive folder: {self.archive_dir}\")\n",
        "            print(f\"üì¶ ZIP file: {zip_filename}\")\n",
        "            print(f\"üñºÔ∏è  Screenshots: screenshot.png, screenshot_full.png\")\n",
        "            print(f\"üìÑ Main file: index.html\")\n",
        "            print(f\"üìä Total assets downloaded: {len(self.downloaded_urls)}\")\n",
        "            print(f\"üìè Page dimensions: {metadata['page_width']}x{metadata['page_height']}px\")\n",
        "\n",
        "            return self.archive_dir, zip_filename\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error creating archive: {str(e)}\")\n",
        "            return None, None\n",
        "        finally:\n",
        "            if hasattr(self, 'driver'):\n",
        "                self.driver.quit()\n",
        "\n",
        "# Enhanced input section\n",
        "print(\"üåê Enhanced Offline Webpage Archiver\")\n",
        "print(\"üìã Features: Lazy loading support, loading screen detection, dynamic content\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "url = input(\"Enter the URL to archive: \").strip()\n",
        "\n",
        "if url:\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'https://' + url\n",
        "\n",
        "    # Get optional parameters\n",
        "    print(\"\\n‚öôÔ∏è Optional Settings:\")\n",
        "    wait_time_input = input(\"Max wait time for page load (default 10s): \").strip()\n",
        "    wait_time = int(wait_time_input) if wait_time_input.isdigit() else 10\n",
        "\n",
        "    scroll_pause_input = input(\"Pause between scrolls (default 2s): \").strip()\n",
        "    scroll_pause = float(scroll_pause_input) if scroll_pause_input.replace('.', '').isdigit() else 2.0\n",
        "\n",
        "    content_selectors_input = input(\"Specific content selectors to wait for (comma-separated, optional): \").strip()\n",
        "    content_selectors = [s.strip() for s in content_selectors_input.split(',')] if content_selectors_input else None\n",
        "\n",
        "    print(f\"\\nüöÄ Starting enhanced archive process for: {url}\")\n",
        "    print(f\"‚è∞ Wait time: {wait_time}s, Scroll pause: {scroll_pause}s\")\n",
        "\n",
        "    # Create archiver and start process\n",
        "    archiver = EnhancedWebpageArchiver(url, wait_time, scroll_pause)\n",
        "    archive_dir, zip_file = archiver.create_archive(content_selectors)\n",
        "\n",
        "    if archive_dir:\n",
        "        print(f\"\\nüìã Archive Summary:\")\n",
        "        print(f\"   ‚Ä¢ Open 'index.html' in the archive folder to view offline\")\n",
        "        print(f\"   ‚Ä¢ All assets (CSS, JS, images, fonts) are included\")\n",
        "        print(f\"   ‚Ä¢ Lazy-loaded content has been triggered and captured\")\n",
        "        print(f\"   ‚Ä¢ Loading screens were detected and waited for\")\n",
        "        print(f\"   ‚Ä¢ Screenshots show the final loaded state\")\n",
        "        print(f\"   ‚Ä¢ ZIP file created for easy sharing\")\n",
        "\n",
        "        # List contents\n",
        "        print(f\"\\nüìÇ Archive contents:\")\n",
        "        for root, dirs, files in os.walk(archive_dir):\n",
        "            level = root.replace(archive_dir, '').count(os.sep)\n",
        "            indent = ' ' * 2 * level\n",
        "            print(f\"{indent}{os.path.basename(root)}/\")\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            for file in files[:10]:  # Limit display\n",
        "                print(f\"{subindent}{file}\")\n",
        "            if len(files) > 10:\n",
        "                print(f\"{subindent}... and {len(files) - 10} more files\")\n",
        "    else:\n",
        "        print(\"‚ùå Archive process failed. Please check the URL and try again.\")\n",
        "else:\n",
        "    print(\"‚ùå Please provide a valid URL.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yFpCw8GQ0zY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Page"
      ],
      "metadata": {
        "id": "YSoLeNhz31e0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prKt8mUKuNBA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Webpage Archiver\n",
        "#@markdown Enter a URL to save it as a complete offline archive with all assets, rewritten links, and screenshots.\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from zipfile import ZipFile\n",
        "from IPython.display import HTML, display\n",
        "import threading\n",
        "import queue\n",
        "\n",
        "# Configuration\n",
        "URL = \"\"  #@param {type:\"string\"}\n",
        "MAX_DEPTH = 1  #@param {type:\"integer\"}\n",
        "DELAY = 1  #@param {type:\"number\"}  # Delay between requests in seconds\n",
        "WAIT_TIME = 3  #@param {type:\"number\"}  # Time to wait for page to load in seconds\n",
        "INCLUDE_EXTERNAL = False  #@param {type:\"boolean\"}  # Include external domains\n",
        "SCREENSHOT_WIDTH = 1280  #@param {type:\"integer\"}  # Screenshot width in pixels\n",
        "SCREENSHOT_HEIGHT = 900  #@param {type:\"integer\"}  # Screenshot height in pixels\n",
        "\n",
        "# Setup directories\n",
        "!mkdir -p /content/archive\n",
        "!mkdir -p /content/archive/assets\n",
        "!mkdir -p /content/archive/screenshots\n",
        "\n",
        "# Setup Chrome options for Selenium\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument(f'--window-size={SCREENSHOT_WIDTH},{SCREENSHOT_HEIGHT}')\n",
        "\n",
        "# Initialize the browser\n",
        "browser = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Track visited URLs to avoid duplicates\n",
        "visited_urls = set()\n",
        "assets_queue = queue.Queue()\n",
        "pages_to_process = queue.Queue()\n",
        "\n",
        "# Function to sanitize filenames\n",
        "def sanitize_filename(filename):\n",
        "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
        "\n",
        "# Function to get domain from URL\n",
        "def get_domain(url):\n",
        "    parsed_uri = urlparse(url)\n",
        "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
        "    return domain\n",
        "\n",
        "# Function to download assets\n",
        "def download_assets():\n",
        "    while True:\n",
        "        asset_info = assets_queue.get()\n",
        "        if asset_info is None:  # Sentinel value to stop the thread\n",
        "            assets_queue.task_done()\n",
        "            break\n",
        "\n",
        "        asset_url, asset_path, asset_folder = asset_info\n",
        "        try:\n",
        "            response = requests.get(asset_url, stream=True, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                # Create directory if it doesn't exist\n",
        "                os.makedirs(os.path.dirname(asset_path), exist_ok=True)\n",
        "\n",
        "                # Save the asset\n",
        "                with open(asset_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(1024):\n",
        "                        f.write(chunk)\n",
        "                print(f\"Downloaded asset: {asset_url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading asset {asset_url}: {str(e)}\")\n",
        "\n",
        "        assets_queue.task_done()\n",
        "\n",
        "# Function to process a webpage\n",
        "def process_page(url, depth=0):\n",
        "    if url in visited_urls or depth > MAX_DEPTH:\n",
        "        return\n",
        "\n",
        "    visited_urls.add(url)\n",
        "    print(f\"Processing: {url} (Depth: {depth})\")\n",
        "\n",
        "    try:\n",
        "        # Load the page with Selenium for screenshot and JavaScript rendering\n",
        "        browser.get(url)\n",
        "        time.sleep(WAIT_TIME)\n",
        "\n",
        "        # Take screenshot\n",
        "        parsed_url = urlparse(url)\n",
        "        screenshot_path = f\"/content/archive/screenshots/{sanitize_filename(parsed_url.netloc + parsed_url.path)}.png\"\n",
        "        browser.save_screenshot(screenshot_path)\n",
        "        print(f\"Saved screenshot: {screenshot_path}\")\n",
        "\n",
        "        # Get page source after JavaScript execution\n",
        "        page_source = browser.page_source\n",
        "\n",
        "        # Parse with BeautifulSoup\n",
        "        soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "        # Create a local path for the HTML file\n",
        "        parsed_url = urlparse(url)\n",
        "        path = parsed_url.path\n",
        "        if path == '' or path.endswith('/'):\n",
        "            path += 'index.html'\n",
        "        if not path.endswith('.html'):\n",
        "            path += '.html'\n",
        "\n",
        "        local_path = f\"/content/archive/{sanitize_filename(parsed_url.netloc)}{path}\"\n",
        "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "        # Process all links and assets\n",
        "        domain = get_domain(url)\n",
        "\n",
        "        # Process stylesheets\n",
        "        for link in soup.find_all('link', rel='stylesheet'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                asset_url = urljoin(url, href)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"style_{len(visited_urls)}.css\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/css/{asset_filename}\"\n",
        "                link['href'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/css/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"css\"))\n",
        "\n",
        "        # Process scripts\n",
        "        for script in soup.find_all('script'):\n",
        "            src = script.get('src')\n",
        "            if src:\n",
        "                asset_url = urljoin(url, src)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"script_{len(visited_urls)}.js\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/js/{asset_filename}\"\n",
        "                script['src'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/js/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"js\"))\n",
        "\n",
        "        # Process images\n",
        "        for img in soup.find_all('img'):\n",
        "            src = img.get('src')\n",
        "            if src:\n",
        "                asset_url = urljoin(url, src)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"image_{len(visited_urls)}.png\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/images/{asset_filename}\"\n",
        "                img['src'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/images/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"images\"))\n",
        "\n",
        "        # Process other assets (videos, audio, etc.)\n",
        "        for tag in soup.find_all(['video', 'audio', 'source']):\n",
        "            src = tag.get('src')\n",
        "            if src:\n",
        "                asset_url = urljoin(url, src)\n",
        "                asset_filename = os.path.basename(asset_url)\n",
        "                if not asset_filename:\n",
        "                    asset_filename = f\"media_{len(visited_urls)}\"\n",
        "\n",
        "                asset_path = f\"/content/archive/assets/{sanitize_filename(parsed_url.netloc)}/media/{asset_filename}\"\n",
        "                tag['src'] = f\"../assets/{sanitize_filename(parsed_url.netloc)}/media/{asset_filename}\"\n",
        "\n",
        "                assets_queue.put((asset_url, asset_path, \"media\"))\n",
        "\n",
        "        # Process internal links for crawling\n",
        "        if depth < MAX_DEPTH:\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link.get('href')\n",
        "                if href and not href.startswith('#') and not href.startswith('javascript:'):\n",
        "                    new_url = urljoin(url, href)\n",
        "                    new_domain = get_domain(new_url)\n",
        "\n",
        "                    # Only follow links from the same domain unless INCLUDE_EXTERNAL is True\n",
        "                    if new_domain == domain or INCLUDE_EXTERNAL:\n",
        "                        pages_to_process.put((new_url, depth + 1))\n",
        "\n",
        "        # Save the modified HTML\n",
        "        with open(local_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(str(soup))\n",
        "        print(f\"Saved page: {local_path}\")\n",
        "\n",
        "        # Add delay to avoid overwhelming the server\n",
        "        time.sleep(DELAY)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "# Start asset download threads\n",
        "num_download_threads = 5\n",
        "download_threads = []\n",
        "for _ in range(num_download_threads):\n",
        "    thread = threading.Thread(target=download_assets)\n",
        "    thread.start()\n",
        "    download_threads.append(thread)\n",
        "\n",
        "# Add the initial URL to the queue\n",
        "pages_to_process.put((URL, 0))\n",
        "\n",
        "# Process pages\n",
        "try:\n",
        "    while not pages_to_process.empty():\n",
        "        url, depth = pages_to_process.get()\n",
        "        process_page(url, depth)\n",
        "        pages_to_process.task_done()\n",
        "\n",
        "        # Small delay to prevent queue checking from consuming too much CPU\n",
        "        time.sleep(0.1)\n",
        "\n",
        "        # Check if we've reached the maximum number of URLs to prevent infinite loops\n",
        "        if len(visited_urls) > 1000:\n",
        "            print(\"Reached maximum number of URLs (1000). Stopping.\")\n",
        "            break\n",
        "finally:\n",
        "    # Stop all download threads\n",
        "    for _ in range(num_download_threads):\n",
        "        assets_queue.put(None)\n",
        "\n",
        "    # Wait for all download threads to finish\n",
        "    for thread in download_threads:\n",
        "        thread.join()\n",
        "\n",
        "    # Close the browser\n",
        "    browser.quit()\n",
        "\n",
        "# Create a ZIP file of the archive\n",
        "domain_name = sanitize_filename(urlparse(URL).netloc)\n",
        "zip_path = f\"/content/{domain_name}_archive.zip\"\n",
        "with ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(\"/content/archive\"):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, \"/content/archive\")\n",
        "            zipf.write(file_path, arcname)\n",
        "\n",
        "print(f\"Archive created: {zip_path}\")\n",
        "\n",
        "# Create a download link\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n",
        "\n",
        "# Display summary\n",
        "display(HTML(f\"\"\"\n",
        "<h3>Archive Summary</h3>\n",
        "<p><strong>Original URL:</strong> {URL}</p>\n",
        "<p><strong>Pages Archived:</strong> {len(visited_urls)}</p>\n",
        "<p><strong>Archive File:</strong> {domain_name}_archive.zip</p>\n",
        "<p>The archive includes:</p>\n",
        "<ul>\n",
        "  <li>All HTML pages with rewritten links</li>\n",
        "  <li>All assets (CSS, JavaScript, images, etc.)</li>\n",
        "  <li>Screenshots of each page</li>\n",
        "</ul>\n",
        "\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}